{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19515a2f-9267-4d9a-a8ee-90f9a347d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from scipy import stats \n",
    "from extra_data import RunDirectory \n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90710a-7a22-494a-b18d-342da4132ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "\n",
    "def read_run_sync_trains(r, verbose=True):\n",
    "    #clear_output(wait=False) \n",
    "    #sys.stderr.write(f'Processing run {r}...\\n') \n",
    "    run = RunDirectory(path='/pnfs/xfel.eu/exfel/archive/XFEL/raw/SPB/202202/p003046/r%04d' % (r))  \n",
    "    \n",
    "    # The require_all=True ensures we only get trains which exist for all sources\n",
    "    sel = run.select([\n",
    "        ('SPB_XTD9_XGM/XGM/DOOCS:output', 'data.intensitySa1TD'),\n",
    "        ('SA1_XTD2_XGM/XGM/DOOCS:output', 'data.intensitySa1TD'),\n",
    "        ('SPB_IRU_MOTORS/MDL/DATA_SELECT', '*'),\n",
    "        ('SPB_EXP_ZYLA/CAM/1:daqOutput', 'data.image.pixels'),\n",
    "    ], require_all=True)\n",
    "    if(verbose):\n",
    "        sel.info()\n",
    "    \n",
    "    intensity_sase1_extra = sel['SPB_XTD9_XGM/XGM/DOOCS:output', 'data.intensitySa1TD']\n",
    "    intensity_preattenuator_sase1_extra = sel['SA1_XTD2_XGM/XGM/DOOCS:output', 'data.intensitySa1TD']    \n",
    "    motors_x_extra = sel['SPB_IRU_MOTORS/MDL/DATA_SELECT', 'SPB_IRU_INJMOV_MOTOR_X.actualPosition.value']\n",
    "    motors_y_extra = sel['SPB_IRU_MOTORS/MDL/DATA_SELECT', 'SPB_IRU_INJMOV_MOTOR_Y.actualPosition.value']\n",
    "\n",
    "    # Put the data in xarray\n",
    "    motor_x = motors_x_extra.xarray()\n",
    "    motor_y = motors_y_extra.xarray()\n",
    "    xgm = intensity_sase1_extra.xarray()\n",
    "    xgm2 = intensity_preattenuator_sase1_extra.xarray()\n",
    "    \n",
    "    directory = '/gpfs/exfel/u/scratch/SPB/202202/p003046/data' \n",
    "    agipd_frames_per_train = 202 # number of agipd frames per train\n",
    "    with h5py.File(directory+'/r%04d_proc_radavg.h5' % (r)) as rad: \n",
    "        radavg = rad['entry_1']['radialavg'][:]\n",
    "        trainIds = rad['entry_1']['trainId'][:]\n",
    "        q = rad['entry_1']['q'][:]\n",
    "    \n",
    "    # Reshape the arrays from the radavg file to make them by train\n",
    "    radavg = radavg.reshape((-1,agipd_frames_per_train,radavg.shape[1]))\n",
    "    trainIds = trainIds.reshape((-1,agipd_frames_per_train))\n",
    "    # Ensure all the trains are the same length\n",
    "    if (trainIds == trainIds[:,:1]).all() != True:\n",
    "        raise ValueError        \n",
    "    if(verbose):\n",
    "        print(\"Reshaping of radavg successful:\", (trainIds == trainIds[:,:1]).all())\n",
    "    trainIds = trainIds[:,0]\n",
    "    \n",
    "    # Put radavg in an xarray\n",
    "    radavg = xarray.DataArray(data=radavg, dims=('trainId', 'pulseNr', 'q'), \n",
    "                              coords={'trainId': trainIds, 'pulseNr': np.arange(agipd_frames_per_train), 'q': q})\n",
    "\n",
    "    # Find trains which are common between radavg and the rest\n",
    "    common_trains = np.intersect1d(intensity_sase1_extra.train_id_coordinates(), trainIds)\n",
    "    if verbose:\n",
    "        print(\"Found %d common train IDs between radavg and the rest\" % len(common_trains))\n",
    "\n",
    "    # Find the indices of the common trainIds and keep only those\n",
    "    common_train_idx = np.searchsorted(intensity_sase1_extra.train_id_coordinates(),common_trains)\n",
    "    motor_x = motor_x[common_train_idx]\n",
    "    motor_y = motor_y[common_train_idx]\n",
    "    xgm = xgm[common_train_idx]\n",
    "\n",
    "    # Find the indices of the common trainIds for radavg    \n",
    "    common_train_idx = np.searchsorted(radavg.coords['trainId'].data,common_trains)\n",
    "    radavg = radavg[common_train_idx]\n",
    "    \n",
    "    train_ids_eq = (motor_x.coords['trainId'].data == radavg.coords['trainId'].data)\n",
    "    if verbose:\n",
    "        print(\"Train IDs all match:\", train_ids_eq.all())\n",
    "    \n",
    "    return xgm, motor_x, motor_y, radavg, xgm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b962a-523e-49aa-a434-0f0be57e69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sync_pulses(xgm, radavg, xgm2):\n",
    "    # Find out how many X-ray pulses per train\n",
    "\n",
    "    npulses = np.sum(np.nanmean(xgm,axis=0) > 1.0)\n",
    "    # For certain runs we actually receive more than 176 pulses, but the agipd will only record 176   \n",
    "    npulses = min(npulses,176)\n",
    "    agipd_frames_per_pulse = 176//npulses\n",
    "    return xgm[:,:npulses], radavg[:,1:npulses*agipd_frames_per_pulse+1:agipd_frames_per_pulse,:], xgm2[:,:npulses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd72509b-0faf-42d4-9b6b-3c5e8cb3697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_q_range(radavg, q_min = 0.73, q_max = 13, verbose=True):\n",
    "    # q_min and q_max in 1/nm\n",
    "\n",
    "    q = radavg.coords['q'].data\n",
    "    q_min_idx = np.argmax(q > q_min)\n",
    "    q_max_idx = np.argmax(q > q_max)\n",
    "    \n",
    "    if(verbose):\n",
    "        print('Integrating between %g-%g 1/nm' % (q_min,q_max))   \n",
    "    I = xarray.DataArray(data=np.nansum(radavg[:,:,q_min_idx:q_max_idx],axis=2), dims=('trainId', 'pulseNr'), coords={'trainId': radavg.coords['trainId'], 'pulseNr': radavg.coords['pulseNr']})\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1129f6-5b69-4e2a-b218-eb3c996b6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_runs(runs, verbose=False):\n",
    "    comb_xgm = None\n",
    "    comb_motor_x = None\n",
    "    comb_motor_y = None\n",
    "    comb_radavg = None\n",
    "    comb_xgm2 = None\n",
    "    comb_I = None\n",
    "    comb_R2 = None\n",
    "    comb_slopes = None\n",
    "    for run in runs:\n",
    "        xgm, motor_x, motor_y, radavg, xgm2 = read_run_sync_trains(run, verbose=verbose)\n",
    "        xgm, radavg, xgm2 = sync_pulses(xgm, radavg, xgm2)\n",
    "        I = integrate_q_range(radavg, verbose=False)\n",
    "        \n",
    "        if comb_I is None:\n",
    "            comb_I = I\n",
    "        else:\n",
    "            comb_I = xarray.concat((comb_I, I),'trainId')\n",
    "\n",
    "        if comb_xgm is None:\n",
    "            comb_xgm = xgm\n",
    "        else:\n",
    "            comb_xgm = xarray.concat((comb_xgm, xgm),'trainId')\n",
    "\n",
    "    return comb_xgm, comb_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab6f8f7-48b4-4d52-98db-b96f1a83110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fit_trains(xgm, I, n=1, xgm_min = 0):\n",
    "    # Do a linear fit of the intensity I versus the xgm, n consecutive trains at a time\n",
    "    # The default is to do one train at a time (n=1)\n",
    "    slopes = np.zeros(int(np.ceil(xgm.shape[0]/n)))\n",
    "    intercept = np.zeros(int(np.ceil(xgm.shape[0]/n)))\n",
    "    R2 = np.zeros(int(np.ceil(xgm.shape[0]/n)))\n",
    "    RelError = np.zeros(int(np.ceil(xgm.shape[0]/n)))\n",
    "    Pearson_R = np.zeros(int(np.ceil(xgm.shape[0]/n)))\n",
    "    \n",
    "    for i,idx in enumerate(np.arange(0,xgm.shape[0],n)):\n",
    "        x = np.ravel(xgm.data[idx:idx+n])\n",
    "        y = np.ravel(I.data[idx:idx+n])\n",
    "        # Only take points with XGM > xgm_min uJ\n",
    "        mask = x > xgm_min\n",
    "        x = x[mask]\n",
    "        y = y[mask]\n",
    "        # Only take shots with more than 1500 uJ\n",
    "        fit = stats.linregress(x, y)\n",
    "\n",
    "                 \n",
    "        slopes[i] = fit.slope\n",
    "        intercept[i] = fit.intercept\n",
    "        R2[i] = fit.rvalue**2\n",
    "        fit_values = x*fit.slope + fit.intercept\n",
    "        RelError[i] = np.nanmean(np.abs((y - fit_values)/fit_values))\n",
    "    return slopes, intercept, R2, RelError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724528bc-499e-49fe-98d7-140639e6f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_xgm, comb_I = combine_runs([84,85,86], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02315ea0-63a8-4f47-a504-158c3b12ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_xgm_lj, comb_I_lj = combine_runs([195,196], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a9d55-644f-48e6-aeb7-b15f7b856716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The beam drops for a while in the liquid jet run. Remove that part\n",
    "#comb_xgm_lj = np.asarray(np.concatenate((comb_xgm_lj[:4450], comb_xgm_lj[4600:]), axis=0))\n",
    "#comb_I_lj = np.asarray(np.concatenate((comb_I_lj[:4450], comb_I_lj[4600:]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9dad4d-ef50-4912-94cf-dab8056dd244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the first 1500 trains as there's a step change (in position) then\n",
    "comb_xgm_lj =comb_xgm_lj[1500:]\n",
    "comb_I_lj =comb_I_lj[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2594f2e6-142a-4e0a-b4cd-74760cdd9d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the two runs are of the same size\n",
    "comb_xgm = comb_xgm[:comb_xgm_lj.shape[0]]\n",
    "comb_I = comb_I[:comb_I_lj.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b90b25-3bab-45f5-bf7c-133c185ae236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Apply the default theme\n",
    "sns.set()\n",
    "\n",
    "fig_handle = plt.figure(figsize=(7,5))\n",
    "spec_handle = fig_handle.add_gridspec(nrows = 1, ncols = 1)\n",
    "ax_i = fig_handle.add_subplot(spec_handle[0,0])\n",
    "slopes, intercept, R2, _ = linear_fit_trains(comb_xgm, comb_I, n = 50, xgm_min = 1500)\n",
    "x = np.linspace(0,len(comb_xgm),num=len(R2))\n",
    "plt.plot(x,R2,'-',label='GDVN')\n",
    "#plt.plot(x,Pearson_R**2,label='GDVN')\n",
    "#ax_i.set_title('Correlation between scattered I and XGM for groups of 100 consecutive trains')\n",
    "slopes, intercept, R2, _ = linear_fit_trains(comb_xgm_lj, comb_I_lj, n = 50, xgm_min = 1500)\n",
    "x = np.linspace(0,len(comb_xgm_lj),num=len(R2))\n",
    "plt.plot(x,R2,'-',label='Flat Jet')\n",
    "#plt.plot(x,Pearson_R**2,label='Flat Jet')\n",
    "ax_i.set_ylim([0,1]);\n",
    "ax_i.set_ylabel('$R^2$')\n",
    "ax_i.set_xlabel('Train number')\n",
    "ax_i.legend(frameon=False);\n",
    "ax_i.spines['top'].set_visible(False)\n",
    "ax_i.spines['right'].set_visible(False);\n",
    "plt.savefig('../figures/correlation_I_xgm.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb84a41-d69e-473d-b043-017d40753a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "plt.figure()\n",
    "scale = 1.0/(np.mean(comb_I.data)/np.mean(comb_xgm.data))\n",
    "scale_lj = 1.0/(np.mean(comb_I_lj)/np.mean(comb_xgm_lj))\n",
    "npulses = len(comb_I.data.flatten())\n",
    "plt.plot(np.linspace(0,npulses//176, npulses),scale*scipy.ndimage.uniform_filter1d(comb_I.data.flatten()/comb_xgm.data.flatten(), size=8800))\n",
    "plt.plot(np.linspace(0,npulses//176, npulses),float(scale_lj)*scipy.ndimage.uniform_filter1d(comb_I_lj.data.flatten()/comb_xgm_lj.data.flatten(), size=8800))\n",
    "plt.xlabel('Train number')\n",
    "plt.ylabel('Normalized scattered intensity')\n",
    "plt.savefig('../figures/xgm_normalized_I_50_trains.pdf')\n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcaac4-ccfc-46bb-9e88-c49a7208c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.plot(scale*np.mean(comb_I.data, axis=1)/np.mean(comb_xgm.data, axis=1), '.', label='GDVN')\n",
    "plt.plot(scale_lj*np.mean(comb_I_lj, axis=1)/np.mean(comb_xgm_lj, axis=1), '.', label='Liquid Sheet')\n",
    "plt.legend(frameon=False)\n",
    "#plt.plot(np.mean(comb_xgm_lj, axis=1))\n",
    "#plt.plot(np.mean(comb_xgm, axis=1))\n",
    "plt.xlabel('Train number')\n",
    "plt.ylabel('Normalized scattered intensity')\n",
    "plt.savefig('../figures/xgm_normalized_I.pdf')\n",
    "plt.plot()\n",
    "print(\"Standard deviation of GDVN signal - %f\" % (np.std(scale*np.mean(comb_I.data, axis=1)/np.mean(comb_xgm.data, axis=1))))\n",
    "print(\"Standard deviation of Liquid Sheet signal - %f\" % (np.std(scale_lj*np.mean(comb_I_lj, axis=1)/np.mean(comb_xgm_lj, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c786aa-ee1b-4d88-929e-fa0144c09da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xfel (current)",
   "language": "python",
   "name": "xfel-current"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
